{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recommend job from skillsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora, similarities\n",
    "from gensim.models import TfidfModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_skills = [\"Python\", \"Machine Learning\", \"AWS\", \"SQL\", \"GIT\"]\n",
    "\n",
    "candidates_skills = [\n",
    "    [\"Python\", \"Machine Learning\", \"AWS\", \"SQL\", \"Deep Learning\"],\n",
    "    [\"R\", \"Statistics\", \"GIT\",  \"English\"],\n",
    "    [\"Python\", \"Machine Learning\", \"AWS\", \"GIT\", \"English\"]\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jaccard similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def matching_score(job_skills, candidate_skills):\n",
    "    \"\"\"modified the Jaccard similarity to matching score\"\"\"\n",
    "    if job_skills:\n",
    "        commom_skills = set(job_skills) & set(candidate_skills)\n",
    "        score = len(commom_skills) / len(set(job_skills))\n",
    "        return score\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score of candidate 0: 0.800000\n",
      "score of candidate 1: 0.200000\n",
      "score of candidate 2: 0.800000\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    print(\"score of candidate %d: %f\" % (i, matching_score(job_skills, candidates_skills[i])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cosine similarity\n",
    "We need make skill embeddings for all the text. Here I use bag of words + tfidf. You are free to further reduce the vector dimension by using Latent semantic indexing(LSI)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## bulid dictionary and embedding use gensim\n",
    "def embedding(skillsets):\n",
    "    dictionary = corpora.Dictionary(skillsets)\n",
    "    corpus = [dictionary.doc2bow(text) for text in skillsets]\n",
    "    model = TfidfModel(corpus)\n",
    "    index = similarities.MatrixSimilarity(model[corpus])\n",
    "    return dictionary, model, index\n",
    "\n",
    "dictionary, model, index = embedding(candidates_skills)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## compute similarity\n",
    "def compute_sim(skills):\n",
    "    job_query = dictionary.doc2bow(skills)\n",
    "    vec_job = model[job_query]\n",
    "    sims = index[vec_job]\n",
    "    return sims\n",
    "\n",
    "sims = compute_sim(job_skills)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score of candidate 0: 0.730248\n",
      "score of candidate 1: 0.072699\n",
      "score of candidate 2: 0.531179\n"
     ]
    }
   ],
   "source": [
    "for i,s in enumerate(sims):\n",
    "    print(\"score of candidate %d: %f\" % (i, s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score of candidate 0: 0.730248\n",
      "score of candidate 2: 0.531179\n",
      "score of candidate 1: 0.072699\n"
     ]
    }
   ],
   "source": [
    "## sort by score\n",
    "sims_sorted = sorted(enumerate(sims), key=lambda item: -item[1])\n",
    "for i, s in enumerate(sims_sorted):\n",
    "    print(\"score of candidate %d: %f\" % (s[0], s[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Mover’s Distance (WMD)\n",
    "\n",
    "WMD allow us the compare two texts by considering the word similarity. In general we will use the pre-computed word vectors. However, in this use case, we have to find a way to compute the skill vectors. Actually it involves two type of NLP tasks, NER (how to identify skills) and skill embedding (how to generate a vector for each skill). Here I assume that we have already had the skill vectors in hand. Then we just need to call the function:\n",
    "```\n",
    "model.wmdistance(skillset_1, skillset_2)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "- [Gemsim documents](https://radimrehurek.com/gensim/auto_examples/index.html#documentation)\n",
    "- [TFIDF](https://en.wikipedia.org/wiki/Tf%E2%80%93idf)\n",
    "- [Latent semantic indexing](https://nlp.stanford.edu/IR-book/html/htmledition/latent-semantic-indexing-1.html)\n",
    "- Word Mover’s Distance:\n",
    "    - [gensim function](https://radimrehurek.com/gensim/auto_examples/tutorials/run_wmd.html)\n",
    "    - [paper](http://proceedings.mlr.press/v37/kusnerb15.pdf)\n",
    "- [Text similarity](https://medium.com/@adriensieg/text-similarities-da019229c894)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
